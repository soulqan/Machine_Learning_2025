{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956d724e",
   "metadata": {},
   "source": [
    "# Praktikum 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b107a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2858609574691293\n",
      "Epoch 1000, Loss: 0.24992788983786587\n",
      "Epoch 2000, Loss: 0.24708653183985724\n",
      "Epoch 3000, Loss: 0.22397183391992148\n",
      "Epoch 4000, Loss: 0.18352842615819975\n",
      "Epoch 5000, Loss: 0.06086628910855657\n",
      "Epoch 6000, Loss: 0.016429769760676165\n",
      "Epoch 7000, Loss: 0.008466177474943538\n",
      "Epoch 8000, Loss: 0.005533046185646509\n",
      "Epoch 9000, Loss: 0.00405695421162467\n",
      "Prediksi:\n",
      "[[0.06234634]\n",
      " [0.9482167 ]\n",
      " [0.94429026]\n",
      " [0.05526473]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Output akhir\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c5a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.24841371336177276\n",
      "Epoch 1000, Loss: 0.20277905596202914\n",
      "Epoch 2000, Loss: 0.1635686660705854\n",
      "Epoch 3000, Loss: 0.14568069289946073\n",
      "Epoch 4000, Loss: 0.13797247388587094\n",
      "Epoch 5000, Loss: 0.1340970539259281\n",
      "Epoch 6000, Loss: 0.13186487659656584\n",
      "Epoch 7000, Loss: 0.1304476852102731\n",
      "Epoch 8000, Loss: 0.12948204402026217\n",
      "Epoch 9000, Loss: 0.1287883025764806\n",
      "Prediksi:\n",
      "[[0.04242313]\n",
      " [0.94382206]\n",
      " [0.49823494]\n",
      " [0.50631452]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Output akhir\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08a85d6",
   "metadata": {},
   "source": [
    "| Epoch | Loss (Hidden = 2 Neuron) | Loss (Hidden = 3 Neuron) |\n",
    "|-------|----------------------------|----------------------------|\n",
    "| 0     | 0.2858609574691293         | 0.24841371336177276        |\n",
    "| 1000  | 0.24992788983786587        | 0.20277905596202914        |\n",
    "| 2000  | 0.24708653183985724        | 0.1635686660705854         |\n",
    "| 3000  | 0.22397183391992148        | 0.14568069289946073        |\n",
    "| 4000  | 0.18352842615819975        | 0.13797247388587094        |\n",
    "| 5000  | 0.06086628910855657        | 0.1340970539259281         |\n",
    "| 6000  | 0.016429769760676165       | 0.13186487659656584        |\n",
    "| 7000  | 0.008466177474943538       | 0.1304476852102731         |\n",
    "| 8000  | 0.005533046185646509       | 0.12948204402026217        |\n",
    "| 9000  | 0.00405695421162467        | 0.1287883025764806         |\n",
    "\n",
    "Hidden 2 neuron → awalnya loss lebih besar tetapi menurun drastis dan berhasil mempelajari XOR dengan baik.\n",
    "\n",
    "Hidden 3 neuron → loss menurun, tetapi training tidak menemukan solusi XOR (neuron 3 kurang optimal dengan weight random awal tersebut)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a243272d",
   "metadata": {},
   "source": [
    "tugas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e912f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.21385577807110795\n",
      "Epoch 1000, Loss: 0.004886492462497567\n",
      "Epoch 2000, Loss: 0.0017250561421684888\n",
      "Epoch 3000, Loss: 0.0010112517491786626\n",
      "Epoch 4000, Loss: 0.0007060382427301699\n",
      "Epoch 5000, Loss: 0.0005391512713126904\n",
      "Epoch 6000, Loss: 0.00043441904547456163\n",
      "Epoch 7000, Loss: 0.0003628589041289503\n",
      "Epoch 8000, Loss: 0.0003109981048542922\n",
      "Epoch 9000, Loss: 0.0002718178650006128\n",
      "Prediksi ReLU:\n",
      "[[0.01965877]\n",
      " [0.99015145]\n",
      " [0.99026304]\n",
      " [0.01965858]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Aktivasi\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backprop\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        losses.append((epoch, loss))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"Prediksi ReLU:\")\n",
    "print(a2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4337e1f",
   "metadata": {},
   "source": [
    "# Perbandingan Loss XOR (Sigmoid vs ReLU)\n",
    "\n",
    "| Epoch | Sigmoid (2 Neuron) | Sigmoid (3 Neuron) | ReLU (3 Neuron) |\n",
    "|-------|----------------------|----------------------|------------------|\n",
    "| 0     | 0.2858609574691293   | 0.24841371336177276  | 0.2984166617226946 |\n",
    "| 1000  | 0.24992788983786587  | 0.20277905596202914  | 0.1263525736522512 |\n",
    "| 2000  | 0.24708653183985724  | 0.1635686660705854   | 0.1254420887788794 |\n",
    "| 3000  | 0.22397183391992148  | 0.14568069289946073  | 0.12524743534526797 |\n",
    "| 4000  | 0.18352842615819975  | 0.13797247388587094  | 0.12516423460375903 |\n",
    "| 5000  | 0.06086628910855657  | 0.1340970539259281   | 0.12512533267096526 |\n",
    "| 6000  | 0.016429769760676165 | 0.13186487659656584  | 0.12510178710591524 |\n",
    "| 7000  | 0.008466177474943538 | 0.1304476852102731   | 0.12508330777294907 |\n",
    "| 8000  | 0.005533046185646509 | 0.12948204402026217  | 0.1250745868396193 |\n",
    "| 9000  | 0.00405695421162467  | 0.1287883025764806   | 0.12506325476662747 |\n",
    "\n",
    "\n",
    "hasil training ReLU tidak sepenuhnya konvergen untuk XOR, hanya sebagian berhasil (output untuk input (1,0) masih 0.49). Loss juga stagnan di sekitar 0.125, artinya jaringan tidak break symmetry."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
